{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from pprint import pprint\n",
    "import textwrap\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(yaml_file='system_prompts.yaml'):\n",
    "    \"\"\"Load system prompts from a YAML file.\"\"\"\n",
    "    with open(yaml_file, 'r') as file:\n",
    "        prompts = yaml.safe_load(file)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def open_data_file(data_file):\n",
    "    \"\"\"Open a JSON data file and return its contents as a dictionary.\"\"\"\n",
    "\n",
    "    with open(data_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def initialize_model(model_name):\n",
    "    \"\"\"Initialize the model and tokenizer for the given model name.\"\"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def format_papers_for_topic(papers):\n",
    "    \"\"\"Format papers for a specific topic into a structured string.\"\"\"\n",
    "\n",
    "    formatted_output = \"\\nPapers to analyze:\\n\\n\"\n",
    "\n",
    "    papers = sorted(papers, key=lambda x: x['id'])\n",
    "    \n",
    "    for paper in papers:\n",
    "        formatted_output += f\"Paper ID: {paper['id']}\\n\"\n",
    "        formatted_output += f\"Title: {paper['title']}\\n\"\n",
    "        formatted_output += f\"Abstract: {paper['abstract']}\\n\"\n",
    "        formatted_output += \"-\" * 80 + \"\\n\\n\"\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "\n",
    "def create_conversation_messages(system_prompt, data):\n",
    "    \"\"\"Create conversation messages for each topic in the JSON file.\"\"\"\n",
    "\n",
    "    topic_messages = {}\n",
    "    \n",
    "    for topic, papers in data.items():\n",
    "        formatted_papers = format_papers_for_topic(papers)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": formatted_papers}\n",
    "        ]\n",
    "\n",
    "        topic_messages[topic] = messages\n",
    "\n",
    "    return topic_messages\n",
    "\n",
    "\n",
    "def get_topic_stats(topic, papers):\n",
    "    \"\"\"Print the number of papers for a given topic.\"\"\"\n",
    "\n",
    "    num_papers = len(papers)\n",
    "    print(f\"Processing topic: {topic}\")\n",
    "    print(f\"Number of papers in topic: {num_papers}\")\n",
    "\n",
    "\n",
    "def get_number_of_tokens(tokenizer, system_prompt, papers, text):\n",
    "    \"\"\"Calculate the number of tokens for the system prompt, papers, and text.\"\"\"\n",
    "\n",
    "    system_prompt_tokens = tokenizer.tokenize(system_prompt)\n",
    "    paper_tokens = tokenizer.tokenize(papers)\n",
    "    output_tokens = tokenizer.tokenize(text)\n",
    "    total_words = len(text.split())\n",
    "    total_tokens = len(system_prompt_tokens) + len(paper_tokens) + len(output_tokens)\n",
    "    \n",
    "    print(f\"Number of tokens in system prompt: {len(system_prompt_tokens)}\")\n",
    "    print(f\"Number of tokens in papers: {len(paper_tokens)}\")\n",
    "    print(f\"Number of tokens in output: {len(output_tokens)}\")\n",
    "    print(f\"Total number of words in output: {total_words}\")\n",
    "    print(f\"Total number of tokens: {total_tokens}\")\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, messages):\n",
    "    \"\"\"Generate a response from the model based on the provided messages.\"\"\"\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1000,\n",
    "    #     num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def pretty_print_model_response(topic, papers, response, width=100, new_response=False):\n",
    "    \"\"\"Pretty print the model's response with academic formatting.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*width)\n",
    "    if new_response:\n",
    "        print(\"UPDATED SUMMARY FOR TOPIC: \" + topic.upper())\n",
    "    else:\n",
    "        print(f\"TOPIC: {topic.upper()}\")\n",
    "    print(\"=\"*width + \"\\n\")\n",
    "\n",
    "    sections = response.split('\\n\\n')\n",
    "    \n",
    "    for section in sections:\n",
    "        formatted_section = textwrap.fill(\n",
    "            section.strip(),\n",
    "            width=width,\n",
    "            initial_indent=\"\",\n",
    "            subsequent_indent=\"\",\n",
    "            # subsequent_indent=\"    \"\n",
    "        )\n",
    "        print(formatted_section + \"\\n\")\n",
    "    \n",
    "    print(\"References:\")\n",
    "    for paper in papers:\n",
    "        print(f\"[{paper['id']}] {paper['title']}\")\n",
    "    \n",
    "    print(\"=\"*width + \"\\n\")\n",
    "\n",
    "\n",
    "def rewrite_request(model, tokenizer, system_prompt, summary):\n",
    "    \"\"\"Generate a shorter summary if the word limit is exceeded.\"\"\"\n",
    "\n",
    "    print(\"The summary exceeds the word limit of 250 words.\")\n",
    "\n",
    "    rewrite_request = f\"\"\"\n",
    "    The summary provided exceeds the word limit of 250 words. Please revise the summary to be shorter and more concise and adhere to the 250-word maximum limit.\n",
    "    \n",
    "    {system_prompt}\n",
    "    ------\n",
    "    Summary:\n",
    "    {summary}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": rewrite_request}\n",
    "    ]\n",
    "\n",
    "    response = generate_response(model, tokenizer, messages)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def get_summary(data_file, system_prompt, model, tokenizer, response_only=False, print_response=True, show_papers=False):\n",
    "    data = open_data_file(data_file)\n",
    "\n",
    "    topic_messages = create_conversation_messages(system_prompt, data)\n",
    "\n",
    "    for topic, messages in topic_messages.items():\n",
    "        papers = data[topic]\n",
    "\n",
    "        response = generate_response(model, tokenizer, messages)\n",
    "        total_words = len(response.split())\n",
    "\n",
    "        if response_only:\n",
    "            if print_response:\n",
    "                pretty_print_model_response(topic, papers, response)\n",
    "\n",
    "                if total_words > 250:\n",
    "                    response = rewrite_request(model, tokenizer, system_prompt, response)\n",
    "                    pretty_print_model_response(topic, papers, response, new_response=True)\n",
    "            else:\n",
    "                if total_words > 250:\n",
    "                    response = rewrite_request(model, tokenizer, system_prompt, response)\n",
    "\n",
    "                    return response\n",
    "        else:\n",
    "            get_topic_stats(topic, papers)\n",
    "            if show_papers:\n",
    "                print(format_papers_for_topic(papers))\n",
    "\n",
    "            pretty_print_model_response(topic, papers, response)\n",
    "            get_number_of_tokens(tokenizer, system_prompt, format_papers_for_topic(papers), response)\n",
    "\n",
    "            if total_words > 250:\n",
    "                response = rewrite_request(model, tokenizer, system_prompt, response)\n",
    "                pretty_print_model_response(topic, papers, response, new_response=True)\n",
    "                get_number_of_tokens(tokenizer, system_prompt, format_papers_for_topic(papers), response)\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = [\"\"\"\n",
    "Aanalyze the following research papers' titles and abstracts to create a comprehensive summary. Follow these specific guidelines:\n",
    "\n",
    "1. Input Format:\n",
    "For each paper, you will receive:\n",
    "- ID: [Paper ID number]\n",
    "- Title: [Paper Title]\n",
    "- Abstract: [Paper Abstract]\n",
    "\n",
    "2. Summary Requirements:\n",
    "- Generate a cohesive summary of approximately 300 words.\n",
    "- Only use information explicitly stated in the provided abstracts.\n",
    "- Use numeric citations in square brackets [1], [2], etc., corresponding to the paper IDs.\n",
    "- Do not say \"Paper [ID]\", use only the [ID].\n",
    "- Focus on key findings, methodologies, and connections between papers.\n",
    "- Highlight common themes and potential contradictions.\n",
    "- Maintain academic tone and language.\n",
    "\n",
    "3. Citation Rules:\n",
    "- Every claim must be supported by at least one citation using [n] format.\n",
    "- Use only the provided papers as sources.\n",
    "- Multiple citations should be listed in ascending order, separated by commas: [1,3,4].\n",
    "\n",
    "4. Structure:\n",
    "- Begin with a brief overview of the research area.\n",
    "- Group related findings and themes.\n",
    "- Present methodological approaches.\n",
    "- Discuss key conclusions.\n",
    "- Identify potential gaps or areas of consensus.\n",
    "\n",
    "Synthesize the information while maintaining academic integrity and avoiding information not present in the provided abstracts.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Aanalyze the following research papers' titles and abstracts to create a comprehensive summary. Follow these specific guidelines:\n",
    "\n",
    "1. Input Format:\n",
    "For each paper, you will receive:\n",
    "- ID: [Paper ID number]\n",
    "- Title: [Paper Title]\n",
    "- Abstract: [Paper Abstract]\n",
    "\n",
    "2. Summary Requirements:\n",
    "- Generate a cohesive summary of up to 300 words (not exceeding this limit).\n",
    "- Only use information explicitly stated in the provided abstracts.\n",
    "- Use numeric citations in square brackets [1], [2], etc., corresponding to the paper IDs.\n",
    "- Do not say \"Paper [ID]\", use only the [ID].\n",
    "- Focus on key findings, methodologies, and connections between papers.\n",
    "- Highlight common themes and potential contradictions.\n",
    "- Maintain academic tone and language.\n",
    "\n",
    "3. Citation Rules:\n",
    "- Every claim must be supported by at least one citation using [n] format.\n",
    "- Use only the provided papers as sources.\n",
    "- Multiple citations should be listed in ascending order, separated by commas: [1,3,4].\n",
    "\n",
    "Synthesize the information while maintaining academic integrity and avoiding information not present in the provided abstracts.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Aanalyze the following research papers' titles and abstracts to create a comprehensive summary. Follow these specific guidelines:\n",
    "\n",
    "1. Input Format:\n",
    "For each paper, you will receive:\n",
    "- ID: [Paper ID number]\n",
    "- Title: [Paper Title]\n",
    "- Abstract: [Paper Abstract]\n",
    "\n",
    "2. Summary Requirements:\n",
    "- Generate a cohesive summary of 200-250 words STRICTLY. Do not exceed 250 words under any circumstances.\n",
    "- Structure the summary in 2-3 concise paragraphs.\n",
    "- Only use information explicitly stated in the provided abstracts.\n",
    "- Use numeric citations in square brackets [1], [2], etc., corresponding to the paper IDs.\n",
    "- Do not say \"Paper [ID]\", use only the [ID].\n",
    "- Focus on key findings, methodologies, and connections between papers.\n",
    "- Highlight common themes and potential contradictions.\n",
    "- Maintain academic tone and language.\n",
    "\n",
    "3. Citation Rules:\n",
    "- Every claim must be supported by at least one citation using [n] format.\n",
    "- Use only the provided papers as sources.\n",
    "- Multiple citations should be listed in ascending order, separated by commas: [1,3,4].\n",
    "\n",
    "4. Length Enforcement:\n",
    "- Before submitting the summary, count the words.\n",
    "- If the word count exceeds 250, revise by removing less critical details while maintaining key findings.\n",
    "\n",
    "Synthesize the information while maintaining academic integrity and avoiding information not present in the provided abstracts.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Analyze the following research papers' titles and abstracts to create a concise summary. Follow these specific guidelines:\n",
    "\n",
    "1. Input Format:\n",
    "For each paper, you will receive:\n",
    "- ID: [Paper ID number]\n",
    "- Title: [Paper Title]\n",
    "- Abstract: [Paper Abstract]\n",
    "\n",
    "2. Summary Structure and Length:\n",
    "- Write EXACTLY two paragraphs:\n",
    "  * First paragraph: A focused introduction and methodology overview\n",
    "  * Second paragraph: Key findings and conclusions\n",
    "- Keep the summary BRIEF and CONCISE\n",
    "- Prioritize only the most significant findings and connections\n",
    "- Eliminate any redundant or secondary information\n",
    "- Use short, clear sentences\n",
    "\n",
    "3. Content Requirements:\n",
    "- Only use information explicitly stated in the provided abstracts\n",
    "- Use numeric citations in square brackets [1], [2], etc., corresponding to the paper IDs\n",
    "- Do not say \"Paper [ID]\", use only the [ID]\n",
    "- Focus on essential findings and critical connections between papers\n",
    "- Highlight only the most important themes and contradictions\n",
    "- Maintain academic tone and language\n",
    "\n",
    "4. Citation Rules:\n",
    "- Every claim must be supported by at least one citation using [n] format\n",
    "- Use only the provided papers as sources\n",
    "- Multiple citations should be listed in ascending order, separated by commas: [1,3,4]\n",
    "\n",
    "5. Length Control:\n",
    "- Keep sentences concise and focused\n",
    "- Avoid elaboration on minor points\n",
    "- Use precise language instead of lengthy descriptions\n",
    "- Eliminate redundant citations when multiple sources support the same point\n",
    "\n",
    "Synthesize the information while maintaining academic integrity and avoiding information not present in the provided abstracts.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Analyze the following research papers' titles and abstracts to create a focused single-paragraph summary. Follow these specific guidelines:\n",
    "\n",
    "1. Input Format:\n",
    "For each paper, you will receive:\n",
    "- ID: [Paper ID number]\n",
    "- Title: [Paper Title]\n",
    "- Abstract: [Paper Abstract]\n",
    "\n",
    "2. Summary Structure:\n",
    "- Create ONE focused paragraph that:\n",
    "  * Begins with a brief context or introduction (1-2 sentences)\n",
    "  * Presents key methodologies and findings in a logical flow\n",
    "  * Ends with the most significant conclusions\n",
    "- Use clear topic sentences and transitions\n",
    "- Maintain a single coherent narrative thread\n",
    "- Keep the summary TIGHT and FOCUSED\n",
    "\n",
    "3. Content Requirements:\n",
    "- Only use information explicitly stated in the provided abstracts\n",
    "- Use numeric citations in square brackets [1], [2], etc., corresponding to the paper IDs\n",
    "- Do not say \"Paper [ID]\", use only the [ID]\n",
    "- Present only the most essential findings and connections\n",
    "- Highlight critical themes and contradictions\n",
    "- Maintain academic tone and language\n",
    "\n",
    "4. Length Control Strategies:\n",
    "- Write approximately 8-10 substantive sentences\n",
    "- Use precise, economical language\n",
    "- Avoid redundant information\n",
    "- Combine related findings from multiple papers in single sentences\n",
    "- Minimize descriptive phrases\n",
    "- Use active voice\n",
    "\n",
    "5. Citation Rules:\n",
    "- Every claim must be supported by at least one citation using [n] format\n",
    "- Use only the provided papers as sources\n",
    "- Multiple citations should be listed in ascending order, separated by commas: [1,3,4]\n",
    "\n",
    "Synthesize the information while maintaining academic integrity and avoiding information not present in the provided abstracts.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7082c7bdff14add9586d11570b9f0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct-1M\"\n",
    "# model_name = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "model, tokenizer = initialize_model(model_name)\n",
    "prompts = load_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: artificial intelligence cyber security\n",
      "Number of papers in topic: 7\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: ARTIFICIAL INTELLIGENCE CYBER SECURITY\n",
      "====================================================================================================\n",
      "\n",
      "The integration of artificial intelligence (AI) and machine learning (ML) into smart cities and\n",
      "internet of things (IoT) systems has emerged as a critical area of research, addressing challenges\n",
      "ranging from energy efficiency to enhanced security [1]. Smart cities leverage AI, ML, and deep\n",
      "reinforcement learning (DRL) to optimize policies and enhance services across various domains,\n",
      "including transportation, healthcare, and energy distribution [1]. These technologies aim to manage\n",
      "urban growth, reduce resource consumption, and improve citizens' quality of life.\n",
      "\n",
      "Intrusion detection systems (IDS) have gained prominence in securing IoT and industrial cyber-\n",
      "physical systems (CPS) against increasingly sophisticated cyber threats. Research has explored the\n",
      "application of deep learning models, such as convolutional neural networks (CNNs) and recurrent\n",
      "neural networks (RNNs), for detecting anomalies and intrusions [2,3]. For instance, a federated\n",
      "learning approach called DeepFed combines CNNs and gated recurrent units (GRUs) to train a\n",
      "comprehensive intrusion detection model across multiple CPSs, ensuring privacy and security during\n",
      "the training phase [2]. Similarly, a deep RNN-based IDS was introduced for Fog computing,\n",
      "demonstrating robust performance in detecting cyber-attacks using the NSL-KDD dataset [3].\n",
      "\n",
      "The creation of realistic and diverse datasets is crucial for evaluating AI-based security systems.\n",
      "One notable contribution is the development of the TON_IoT dataset, which captures real-world\n",
      "behaviors and cyber threat scenarios from IoT services, Windows/Linux-based systems, and network\n",
      "traffic [4]. This dataset has been validated using machine learning algorithms, including gradient\n",
      "boosting machines, random forests, naive Bayes, and deep neural networks, showcasing its utility in\n",
      "assessing the efficacy of AI-based IDS [4].\n",
      "\n",
      "Machine learning techniques, such as support vector machines (SVM), K-nearest neighbors (KNN), and\n",
      "decision trees (DT), have been widely applied in intrusion detection, demonstrating varying levels\n",
      "of success depending on the dataset and normalization techniques employed [5]. Additionally, tree-\n",
      "based models, like the IntruDTree, have been proposed to optimize feature selection and reduce\n",
      "computational complexity, achieving high accuracy and efficiency in detecting cyber threats [6].\n",
      "\n",
      "Despite these advancements, several challenges remain. Ensuring privacy and scalability in federated\n",
      "learning models [2], improving the robustness of IDS in dynamic network environments [3], and\n",
      "addressing the ethical and security concerns inherent in AI-integrated systems are ongoing areas of\n",
      "focus [7]. The collective efforts of researchers in this field highlight the necessity of\n",
      "interdisciplinary collaboration to address these multifaceted challenges and realize the full\n",
      "potential of AI and ML in smart cities and IoT ecosystems.\n",
      "\n",
      "Common themes across the papers include the reliance on deep learning models for enhanced detection\n",
      "capabilities, the importance of creating diverse and realistic datasets for evaluation, and the need\n",
      "for privacy-preserving methods in federated learning architectures. However, there is no direct\n",
      "contradiction between the papers; rather, they complement each other by focusing on different\n",
      "aspects of AI and ML application in smart cities and IoT security. The overall direction is towards\n",
      "more sophisticated, adaptable, and secure systems that can effectively handle the complexities of\n",
      "modern urban and network environments.\n",
      "\n",
      "References:\n",
      "[1] Applications of Artificial Intelligence and Machine learning in smart cities\n",
      "[2] DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber–Physical Systems\n",
      "[3] Deep recurrent neural network for IoT intrusion detection system\n",
      "[4] A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets\n",
      "[5] Machine learning methods for cyber security intrusion detection: Datasets and comparative study\n",
      "[6] IntruDTree: A Machine Learning-Based Cyber Security Intrusion Detection Model\n",
      "[7] Artificial intelligence in Internet of things\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 265\n",
      "Number of tokens in papers: 2020\n",
      "Number of tokens in output: 627\n",
      "Total number of words in output: 482\n",
      "Total number of tokens: 2912\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.1.json\"\n",
    "system_prompt = prompts['prompts']['comprehensive_300']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: data visualization\n",
      "Number of papers in topic: 6\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "The field of scientific data visualization encompasses tools that assist researchers in interpreting\n",
      "complex data generated by various experimental techniques, from crystallography to genomics. These\n",
      "tools aim to enhance understanding through effective representation and analysis of intricate\n",
      "datasets [1,2,3,4,5,6].\n",
      "\n",
      "One prominent tool for crystallographic studies is VESTA, which has been upgraded to VESTA 3. This\n",
      "version introduces advanced features such as external morphology drawing, superimposition of\n",
      "structural models and volumetric data, calculation of electron and nuclear densities, and enhanced\n",
      "performance in rendering isosurfaces and slice calculations [1]. Another tool, OVITO, specializes in\n",
      "processing and visualizing atomistic simulation data from molecular dynamics and Monte Carlo\n",
      "simulations, offering unique analysis and animation capabilities through an easy-to-use interface\n",
      "[2].\n",
      "\n",
      "For the analysis of X-ray diffraction data, the HKL package offers comprehensive statistical\n",
      "evaluation and visualization tools that help operators assess data quality and monitor progress in\n",
      "structural refinement [3]. Meanwhile, deepTools2, an updated web server based on Galaxy, supports a\n",
      "wide range of bioinformatics workflows, including quality control, normalization, and integration of\n",
      "deep-sequencing data, thereby facilitating the interpretation of large-scale genomic data [4].\n",
      "Similarly, the Integrative Genomics Viewer (IGV) is designed to handle large, diverse genomic\n",
      "datasets, integrating both array-based and next-generation sequencing data, as well as clinical and\n",
      "phenotypic information [5].\n",
      "\n",
      "Dimensionality reduction techniques play a crucial role in simplifying the complexity of high-\n",
      "dimensional data, particularly in single-cell studies. UMAP, a nonlinear dimensionality-reduction\n",
      "method, has demonstrated superior performance in terms of speed, reproducibility, and meaningful\n",
      "clustering of cell types across various biological datasets, making it a powerful tool for\n",
      "visualizing and interpreting single-cell data [6].\n",
      "\n",
      "Common themes among these tools include the emphasis on user-friendly interfaces, integration of\n",
      "diverse data types, and the facilitation of complex data analysis. However, there is a notable gap\n",
      "in the direct comparison of different visualization tools across varying domains, suggesting a need\n",
      "for standardized evaluation frameworks to guide researchers in selecting the most appropriate tool\n",
      "for their specific needs [2,4,5,6]. Additionally, the continuous development and improvement of\n",
      "these tools highlight the dynamic nature of scientific data visualization, driven by advancements in\n",
      "experimental techniques and computational resources.\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 265\n",
      "Number of tokens in papers: 1258\n",
      "Number of tokens in output: 476\n",
      "Total number of words in output: 350\n",
      "Total number of tokens: 1999\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.2.json\"\n",
    "system_prompt = prompts['prompts']['comprehensive_300']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: artificial intelligence cyber security\n",
      "Number of papers in topic: 7\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: ARTIFICIAL INTELLIGENCE CYBER SECURITY\n",
      "====================================================================================================\n",
      "\n",
      "The integration of artificial intelligence (AI) and machine learning (ML) technologies is pivotal in\n",
      "addressing the multifaceted challenges faced by smart cities and Internet of Things (IoT) networks\n",
      "[1]. These technologies are crucial for optimizing policy design, enhancing cyber-security,\n",
      "improving energy efficiency, and ensuring effective healthcare services within smart city frameworks\n",
      "[1]. Specifically, AI and ML enable the development of robust intrusion detection systems (IDS)\n",
      "capable of safeguarding industrial cyber-physical systems (CPS) from sophisticated cyber threats\n",
      "[2].\n",
      "\n",
      "Federated learning approaches, like DeepFed, have emerged as promising solutions for training IDS\n",
      "models across multiple CPS instances without compromising data privacy [2]. By leveraging\n",
      "convolutional neural networks (CNNs) and gated recurrent units (GRUs), DeepFed constructs a\n",
      "comprehensive intrusion detection model that effectively identifies various types of cyber threats\n",
      "[2]. Similarly, recurrent neural networks (RNNs) have been employed in an automated IDS tailored for\n",
      "Fog computing environments, demonstrating stability and robustness against cyber-attacks through\n",
      "rigorous evaluation metrics [3].\n",
      "\n",
      "The creation of heterogeneous datasets, such as the TON_IoT dataset, provides a valuable resource\n",
      "for evaluating AI-based security systems in IoT networks [4]. These datasets capture real-world\n",
      "normal and attack scenarios, offering diverse legitimate and anomalous patterns that enhance the\n",
      "validation of new security solutions [4]. Additionally, traditional machine learning algorithms,\n",
      "such as Support Vector Machines (SVM), K-Nearest Neighbor (KNN), and Decision Trees (DT), continue\n",
      "to be utilized alongside advanced techniques, often achieving competitive performance in intrusion\n",
      "detection tasks [5].\n",
      "\n",
      "Machine learning-based intrusion detection models, such as IntruDTree, further emphasize the\n",
      "importance of feature selection and dimensionality reduction in achieving both high prediction\n",
      "accuracy and reduced computational complexity [6]. These models prioritize feature importance to\n",
      "optimize performance while minimizing resource consumption, a critical consideration for resource-\n",
      "constrained environments like IoT networks [6].\n",
      "\n",
      "Despite these advancements, the integration of AI and ML in IoT and CPS remains fraught with\n",
      "challenges, particularly concerning security and ethical considerations [7]. Ensuring robust\n",
      "security measures, including privacy-preserving federated learning and comprehensive dataset\n",
      "generation, is essential for mitigating the risks associated with AI-driven systems [2,3,4].\n",
      "Ultimately, the successful implementation of AI and ML in smart cities and IoT networks hinges on\n",
      "balancing technological innovation with stringent security practices to foster trust and adoption\n",
      "among users.\n",
      "\n",
      "References:\n",
      "[1] Applications of Artificial Intelligence and Machine learning in smart cities\n",
      "[2] DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber–Physical Systems\n",
      "[3] Deep recurrent neural network for IoT intrusion detection system\n",
      "[4] A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets\n",
      "[5] Machine learning methods for cyber security intrusion detection: Datasets and comparative study\n",
      "[6] IntruDTree: A Machine Learning-Based Cyber Security Intrusion Detection Model\n",
      "[7] Artificial intelligence in Internet of things\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 229\n",
      "Number of tokens in papers: 2020\n",
      "Number of tokens in output: 482\n",
      "Total number of words in output: 363\n",
      "Total number of tokens: 2731\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.1.json\"\n",
    "system_prompt = prompts['prompts']['strict_300']['content']\n",
    "get_summary(data_file, system_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: data visualization\n",
      "Number of papers in topic: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "The field of scientific data visualization encompasses a wide range of applications, from\n",
      "crystallography and atomic simulations to genomic and single-cell data analysis. VESTA 3 [1] offers\n",
      "advanced three-dimensional visualization capabilities for crystallographic studies, allowing users\n",
      "to draw crystal morphologies, overlay multiple structural models, and calculate electron and nuclear\n",
      "densities. This tool is particularly useful for researchers requiring detailed visual insights into\n",
      "complex molecular structures.\n",
      "\n",
      "In contrast, OVITO [2] focuses on post-processing atomistic simulation data, providing unique\n",
      "analysis and editing functionalities through an easy-to-use graphical interface. Its flexibility,\n",
      "supported by Python scripting and plugin capabilities, makes it suitable for a broader spectrum of\n",
      "atomic-scale simulations, enhancing its versatility compared to VESTA.\n",
      "\n",
      "For X-ray diffraction data, the HKL package [3] provides comprehensive tools for data reduction,\n",
      "offering statistical measures like χ² and R-merge to assess data quality. These tools also\n",
      "facilitate the visualization of the data collection process, ensuring accuracy and helping\n",
      "researchers determine optimal data collection strategies.\n",
      "\n",
      "DeepTools2 [4] builds upon its predecessor by offering enhanced functionalities for deep-sequencing\n",
      "data analysis, enabling users to perform integrated analyses and visualize results within a Galaxy\n",
      "framework. This tool streamlines the analysis pipeline for large-scale sequencing projects, making\n",
      "it accessible to both web service users and those deploying it locally.\n",
      "\n",
      "In genomics, IGV [5] stands out for its ability to handle large, heterogeneous datasets efficiently.\n",
      "By supporting both array-based and next-generation sequencing data, IGV facilitates integrative\n",
      "analysis and exploration, especially when combined with clinical and phenotypic data. This\n",
      "capability underscores its utility in comprehensive genomic studies.\n",
      "\n",
      "Lastly, UMAP [6] demonstrates superior performance in visualizing high-dimensional single-cell data,\n",
      "providing faster run times and higher reproducibility compared to other dimensionality-reduction\n",
      "techniques. This method enhances the interpretability of complex single-cell datasets, aiding\n",
      "researchers in understanding cellular compositions and interactions.\n",
      "\n",
      "Overall, these tools highlight the importance of tailored visualization solutions across diverse\n",
      "scientific domains, each addressing unique challenges and offering distinct advantages in data\n",
      "representation and analysis. [1,2,3,4,5,6]\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 229\n",
      "Number of tokens in papers: 1258\n",
      "Number of tokens in output: 436\n",
      "Total number of words in output: 321\n",
      "Total number of tokens: 1923\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.2.json\"\n",
    "system_prompt = prompts['prompts']['strict_300']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TOPIC: ARTIFICIAL INTELLIGENCE CYBER SECURITY\n",
      "====================================================================================================\n",
      "\n",
      "The application of artificial intelligence (AI) and machine learning (ML) in smart cities aims to\n",
      "enhance urban efficiency, energy consumption, environmental sustainability, and citizen welfare\n",
      "through advanced information and communication technology (ICT) integration [1]. Key areas\n",
      "benefiting from AI include intelligent transportation systems, energy grid optimization, and\n",
      "healthcare services, all of which rely on predictive analytics and autonomous decision-making\n",
      "capabilities.\n",
      "\n",
      "In the realm of cybersecurity, particularly for industrial cyber-physical systems (CPSs) and IoT\n",
      "networks, federated deep learning models have shown promise in mitigating attacks without\n",
      "compromising privacy. For instance, DeepFed integrates convolutional neural networks (CNNs) and\n",
      "gated recurrent units (GRUs) to detect cyber threats in industrial CPSs, utilizing a privacy-\n",
      "preserving federated learning framework [2]. Similarly, another approach leverages multi-layered\n",
      "recurrent neural networks (RNNs) for intrusion detection in Fog computing environments,\n",
      "demonstrating robust performance against various cyber-attack vectors [3].\n",
      "\n",
      "The development of comprehensive datasets, such as the TON_IoT, is crucial for validating AI-based\n",
      "security systems. These datasets capture real-world network behaviors and attack scenarios across\n",
      "different computing layers, providing valuable ground truth for model evaluation [4]. Comparative\n",
      "studies on various machine learning algorithms, including SVM, KNN, and decision trees, highlight\n",
      "their efficacy in intrusion detection, although deep learning models often outperform traditional\n",
      "methods in terms of accuracy and robustness [5,6].\n",
      "\n",
      "Despite these advancements, several challenges remain, including the need for continuous monitoring,\n",
      "adaptive learning, and addressing ethical concerns related to privacy and data security in AI-\n",
      "powered IoT ecosystems [1,7]. Future research should focus on integrating federated learning\n",
      "frameworks with scalable architectures capable of handling massive datasets, ensuring both\n",
      "operational efficiency and security in smart cities and industrial CPSs.\n",
      "\n",
      "References:\n",
      "[1] Applications of Artificial Intelligence and Machine learning in smart cities\n",
      "[2] DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber–Physical Systems\n",
      "[3] Deep recurrent neural network for IoT intrusion detection system\n",
      "[4] A new distributed architecture for evaluating AI-based security systems at the edge: Network TON_IoT datasets\n",
      "[5] Machine learning methods for cyber security intrusion detection: Datasets and comparative study\n",
      "[6] IntruDTree: A Machine Learning-Based Cyber Security Intrusion Detection Model\n",
      "[7] Artificial intelligence in Internet of things\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.1.json\"\n",
    "system_prompt = prompts['prompts']['strict_300']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer, response_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "Recent advancements in scientific visualization have introduced powerful tools capable of handling\n",
      "diverse data types across various domains, enhancing analysis and interpretation. VESTA 3 [1] stands\n",
      "out for its comprehensive three-dimensional visualization capabilities, enabling the depiction of\n",
      "crystal structures, volumetric data, and morphologies alongside advanced density calculations and\n",
      "enhanced performance improvements. This tool facilitates detailed analysis and presentation of\n",
      "complex structural data, making it particularly useful for crystallographic studies.\n",
      "\n",
      "OVITO, an open-source 3D visualization software [2], caters specifically to the needs of atomistic\n",
      "simulation data, integrating unique analysis, editing, and animation functionalities. Its\n",
      "extensibility through a plugin interface and control via Python scripts offer flexibility, aligning\n",
      "well with the computational demands of molecular dynamics and Monte Carlo simulations.\n",
      "\n",
      "In contrast, the focus shifts towards bioinformatics with the introduction of deepTools2 [4], a web\n",
      "server designed for deep-sequencing data analysis. This platform, built within the Galaxy framework,\n",
      "offers robust tools for quality control, normalization, and visualization, supporting integrative\n",
      "analyses critical for bioinformatics research. Similarly, IGV [5] addresses the challenge of\n",
      "visualizing large, heterogeneous genomic datasets, providing high-performance visualization and\n",
      "exploration tailored for both local and remote data integration, essential for integrative genomics\n",
      "studies.\n",
      "\n",
      "The application of UMAP [6] in single-cell data analysis demonstrates its effectiveness in\n",
      "dimensionality reduction, offering superior performance and cluster organization compared to other\n",
      "techniques. This nonlinear method significantly aids in the visualization and interpretation of\n",
      "high-dimensional biological data, facilitating more accurate insights into cellular compositions.\n",
      "\n",
      "While VESTA 3 and OVITO emphasize structural and atomistic visualization, deepTools2 and IGV target\n",
      "bioinformatics, highlighting the versatility of modern visualization tools across different\n",
      "scientific disciplines. Despite these differences, all tools share a common goal: to enhance the\n",
      "interpretability of complex datasets through advanced visualization techniques, thereby advancing\n",
      "research in their respective fields [1,2,4,5,6].\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.2.json\"\n",
    "system_prompt = prompts['prompts']['strict_300']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer, response_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: data visualization\n",
      "Number of papers in topic: 6\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "Visualization and analysis of complex scientific data across various domains have seen significant\n",
      "advancements through specialized tools. VESTA [1], a three-dimensional visualization system,\n",
      "enhances crystallographic studies with features like morphology drawing, superimposition of\n",
      "structural models, and advanced density calculations, facilitating comprehensive analysis of crystal\n",
      "structures. Similarly, OVITO [2] provides a versatile platform for post-processing atomistic\n",
      "simulation data, incorporating unique analysis and animation capabilities accessible via a user-\n",
      "friendly interface. Both tools emphasize graphical representation and ease of use, catering to\n",
      "diverse scientific inquiries.\n",
      "\n",
      "In contrast, deepTools2 [4] focuses on deep-sequencing data, offering a robust set of tools for\n",
      "quality control, normalization, and integrative analysis within a Galaxy-based framework. This web\n",
      "server is particularly valuable for users requiring extensive computational resources for large-\n",
      "scale genomic data manipulation. The Integrative Genomics Viewer (IGV) [5] addresses the challenge\n",
      "of handling large, heterogeneous genomic datasets, providing efficient visualization and integration\n",
      "capabilities for both array-based and next-generation sequencing data. These tools highlight the\n",
      "importance of scalable and adaptable interfaces in managing complex biological information [4,5].\n",
      "\n",
      "Dimensionality reduction techniques like UMAP [6] offer rapid and reproducible methods for\n",
      "visualizing single-cell data, demonstrating superior performance compared to existing tools in\n",
      "organizing cell clusters meaningfully. By leveraging non-linear projections, UMAP simplifies high-\n",
      "dimensional data representations, aiding in the identification of intricate biological patterns.\n",
      "Together, these tools underscore the evolving landscape of scientific visualization, where\n",
      "versatility, scalability, and interpretability are paramount [1,2,4,5,6].\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 289\n",
      "Number of tokens in papers: 1258\n",
      "Number of tokens in output: 320\n",
      "Total number of words in output: 230\n",
      "Total number of tokens: 1867\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.2.json\"\n",
    "system_prompt = prompts['prompts']['strict_250']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: data visualization\n",
      "Number of papers in topic: 6\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "The first paragraph focuses on advanced visualization and analysis tools designed for handling\n",
      "complex scientific data. VESTA 3 introduces a comprehensive suite of features for three-dimensional\n",
      "visualization of crystal structures, electron density calculations, and volumetric data, enabling\n",
      "detailed crystallographic studies [1]. Meanwhile, OVITO offers a versatile platform for post-\n",
      "processing atomistic simulation data, integrating unique analysis and animation capabilities within\n",
      "an accessible interface [2]. These tools are complemented by deepTools2, which enhances the analysis\n",
      "of deep-sequencing data through a robust set of tools for quality control, normalization, and\n",
      "visualization, all accessible via a web server or Galaxy framework [4]. Additionally, the\n",
      "Integrative Genomics Viewer (IGV) addresses the challenges of visualizing large, diverse genomic\n",
      "datasets, supporting both array-based and next-generation sequencing data, along with clinical and\n",
      "phenotypic data integration [5].\n",
      "\n",
      "In the second paragraph, key findings emphasize the superior performance and versatility of these\n",
      "tools in various domains. VESTA 3's enhanced bond-search algorithm and performance improvements in\n",
      "rendering isosurfaces offer significant advancements in crystallographic analysis [1]. OVITO's\n",
      "Python scripting and plug-in interface facilitate customization and extension, making it ideal for\n",
      "complex atomistic simulations [2]. DeepTools2's updated functionalities improve data visualization\n",
      "and interpretation, particularly through its enhanced clustering and visualization approaches [4].\n",
      "IGV stands out for its ability to handle large, heterogeneous datasets efficiently, facilitating the\n",
      "integration of diverse genomic information [5]. Lastly, UMAP demonstrates exceptional speed,\n",
      "reproducibility, and meaningful cluster organization in single-cell data analysis, outperforming\n",
      "other dimensionality reduction techniques [6]. Collectively, these tools significantly advance the\n",
      "field by providing powerful, efficient, and user-friendly solutions for complex data visualization\n",
      "and analysis.\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 328\n",
      "Number of tokens in papers: 1258\n",
      "Number of tokens in output: 358\n",
      "Total number of words in output: 259\n",
      "Total number of tokens: 1944\n",
      "The summary exceeds the word limit of 250 words.\n",
      "\n",
      "====================================================================================================\n",
      "UPDATED SUMMARY FOR TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "The first paragraph highlights the development of specialized tools for managing intricate\n",
      "scientific data. VESTA 3 provides advanced features for three-dimensional visualization of crystal\n",
      "structures, electron density, and volumetric data, enhancing crystallographic studies [1]. OVITO\n",
      "stands out for its capabilities in analyzing and animating atomistic simulation data, offering a\n",
      "flexible interface for customization and extension [2]. DeepTools2 refines the analysis of deep-\n",
      "sequencing data with improved tools for quality control, normalization, and visualization,\n",
      "accessible via web servers or the Galaxy framework [4]. The Integrative Genomics Viewer (IGV)\n",
      "tackles the complexity of large, diverse genomic datasets, integrating array-based and next-\n",
      "generation sequencing data alongside clinical and phenotypic information [5].\n",
      "\n",
      "In the second paragraph, key findings underscore the effectiveness and versatility of these tools\n",
      "across different fields. VESTA 3’s optimized bond-search algorithm and faster isosurface rendering\n",
      "provide significant enhancements in crystallographic analysis [1]. OVITO’s Python scripting and\n",
      "plug-in interface enable users to tailor analyses to their specific needs, making it suitable for\n",
      "complex simulations [2]. DeepTools2’s upgraded features, including better clustering and\n",
      "visualization methods, improve data interpretation [4]. IGV excels in efficiently handling large,\n",
      "heterogeneous datasets, facilitating comprehensive genomic data integration [5]. UMAP demonstrates\n",
      "superior speed, reproducibility, and effective cluster organization in single-cell data analysis,\n",
      "outperforming other dimensionality reduction techniques [6]. These tools collectively advance\n",
      "scientific research by offering powerful, efficient, and user-friendly solutions for complex data\n",
      "visualization and analysis.\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 328\n",
      "Number of tokens in papers: 1258\n",
      "Number of tokens in output: 317\n",
      "Total number of words in output: 226\n",
      "Total number of tokens: 1903\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.2.json\"\n",
    "system_prompt = prompts['prompts']['two_paragraph']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic: data visualization\n",
      "Number of papers in topic: 6\n",
      "\n",
      "====================================================================================================\n",
      "TOPIC: DATA VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "Visualization and analysis of complex data across various scientific domains have seen significant\n",
      "advancements with the development of specialized tools. VESTA 3 offers comprehensive three-\n",
      "dimensional visualization capabilities for crystallographic studies, enabling the integration of\n",
      "structural models, volumetric data, and crystal faces, alongside enhanced performance in rendering\n",
      "isosurfaces [1]. In parallel, OVITO provides a robust platform for post-processing atomistic\n",
      "simulation data, incorporating unique analysis and animation functionalities, while remaining\n",
      "accessible and extensible through scripting and plugin interfaces [2]. For X-ray diffraction data,\n",
      "the HKL package introduces advanced statistical and visualization tools that assist in monitoring\n",
      "the progress of data collection and reduction, ensuring accurate error estimates and facilitating\n",
      "the decision-making process for structure solution [3]. In the realm of genomics, the Integrative\n",
      "Genomics Viewer (IGV) stands out for its ability to handle large, diverse datasets efficiently,\n",
      "supporting both array-based and next-generation sequencing data, and facilitating the integration of\n",
      "clinical and phenotypic information [5]. Additionally, deepTools2 enhances the analysis of deep-\n",
      "sequencing data with a comprehensive suite of tools for quality control, normalization, and\n",
      "integrative analyses, accessible via a Galaxy-based web server [4]. Finally, UMAP emerges as a\n",
      "powerful dimensionality reduction technique for visualizing single-cell data, offering superior\n",
      "cluster organization, reproducibility, and computational efficiency compared to other methods [6].\n",
      "These tools collectively address the growing complexity and scale of modern scientific data,\n",
      "providing robust solutions for visualization and interpretation across multiple disciplines.\n",
      "\n",
      "References:\n",
      "[1] VESTA 3for three-dimensional visualization of crystal, volumetric and morphology data\n",
      "[2] Visualization and analysis of atomistic simulation data with OVITO–the Open Visualization Tool\n",
      "[3] Processing of X-ray diffraction data collected in oscillation mode\n",
      "[4] deepTools2: a next generation web server for deep-sequencing data analysis\n",
      "[5] Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration\n",
      "[6] Dimensionality reduction for visualizing single-cell data using UMAP\n",
      "====================================================================================================\n",
      "\n",
      "Number of tokens in system prompt: 334\n",
      "Number of tokens in papers: 1258\n",
      "Number of tokens in output: 306\n",
      "Total number of words in output: 229\n",
      "Total number of tokens: 1898\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data.2.json\"\n",
    "system_prompt = prompts['prompts']['single_paragraph']['content']\n",
    "summary = get_summary(data_file, system_prompt, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
